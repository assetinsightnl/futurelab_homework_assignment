{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FutureLab Machine Learning workshop - homework assignment\n",
    "\n",
    "This notebook is the final homework assignment for the FutureLab Machine Learning workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "# Don't touch this code!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: random forest\n",
    "\n",
    "### Dataset\n",
    "\n",
    "For the first assignment we will look at a dataset on default of credit card clients in Taiwan. This data has a binary variable, default payment (Yes = 1, No = 0), as the response variable, and the following 23 variables as explanatory variables: \n",
    "* X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "* X2: Gender (1 = male; 2 = female). \n",
    "* X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "* X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "* X5: Age (year). \n",
    "* X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "* X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n",
    "* X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. \n",
    "\n",
    "### Model\n",
    "\n",
    "We use a random forest to see if we can predict if a person will default on their payment or not.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. How does a random forest work? What are the advantages of using a random forest over a decision tree?\n",
    "2. What do the parameters 'n_estimators' and 'max_depth' mean? Can you find values that push the test set accuracy over 83%?\n",
    "3. Which parameter can you use for regularization and why?\n",
    "4. If you work for a credit card company, which fields in the confusion matrix do you want to minimize?\n",
    "5. Optional: write code to use gradient boosted decision trees instead of a random forest. Does this method perform better? What are the advantages of gradient boosting over a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "# Don't touch this code!\n",
    "df_credit = pd.read_csv('credit.csv', sep=';').drop('ID', axis=1)\n",
    "x = np.array(df_credit.drop('Y', axis=1))\n",
    "y = np.array(df_credit['Y']).flatten()\n",
    "x_train = x[:25000]\n",
    "x_test = x[25000:]\n",
    "y_train = y[:25000]\n",
    "y_test = y[25000:]\n",
    "print(\"First 5 rows of data:\")\n",
    "print(df_credit.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and train decision tree model.\n",
    "tree = RandomForestClassifier(random_state=0, n_estimators=1, max_depth=100)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "# Test the model.\n",
    "# Don't touch this code!\n",
    "acc = tree.score(x_test, y_test)\n",
    "conf_matrix = confusion_matrix(y_test, tree.predict(x_test), labels=[0, 1])\n",
    "df_conf = pd.DataFrame(data=conf_matrix.T)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"---------\")\n",
    "print(\"Confusion matrix (horizontal axis: actual class, vertical axis: predicted class):\")\n",
    "print(df_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: neural networks\n",
    "\n",
    "### Dataset\n",
    "\n",
    "For this assignment we use the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels).\n",
    "\n",
    "### Model\n",
    "\n",
    "We use a neural network to predict the clothing category from the pixel data.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Turn the neural network into a convolutional neural network. What code did you change/add?\n",
    "2. Explain what each layer does, and what its parameters mean.\n",
    "3. What is the danger of using too many epochs to train your model?\n",
    "4. How could we use data augmentation to improve our model? (You don't need to program this, just explain.)\n",
    "5. Optional: can you get up to 90% test set accuracy within 3 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "# Don't touch this code!\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "class_names = np.array(['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])\n",
    "\n",
    "# Show some example figures and labels.\n",
    "# Don't touch this code!\n",
    "print(\"9 training images:\")\n",
    "plt.figure(figsize=(18, 2))\n",
    "for i, x in enumerate(train_images[:9]):\n",
    "    subplot_id = 191 + i\n",
    "    plt.subplot(subplot_id)\n",
    "    plt.imshow(x, cmap=plt.get_cmap('Greys'))\n",
    "plt.show()\n",
    "print(\"Labels:\")\n",
    "print(class_names[train_labels[:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network.\n",
    "# Here, you can add some extra layers to improve the model!\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data.\n",
    "train_images_reshaped = train_images.reshape(60000, 28, 28, 1)\n",
    "model.fit(train_images_reshaped, train_labels, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate convolutional neural network.\n",
    "# Don't touch this code!\n",
    "test_images_reshaped = test_images.reshape(10000, 28, 28, 1)\n",
    "pred_labels = model.predict(test_images_reshaped).argmax(axis=1)\n",
    "model.evaluate(test_images_reshaped, test_labels)\n",
    "\n",
    "# Show the first 9 images, true labels and predicted labels.\n",
    "# Don't touch this code!\n",
    "print(\"-----------------\")\n",
    "print(\"9 testing images:\")\n",
    "plt.figure(figsize=(18, 2))\n",
    "for i, x in enumerate(test_images[10:19]):\n",
    "    subplot_id = 191 + i\n",
    "    plt.subplot(subplot_id)\n",
    "    plt.imshow(x, cmap=plt.get_cmap('Greys'))\n",
    "plt.show()\n",
    "print(\"True labels:\")\n",
    "print(class_names[test_labels[10:19]])\n",
    "print(\"Predicted labels:\")\n",
    "print(class_names[pred_labels[10:19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
